{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvP1eNN_pExM"
      },
      "source": [
        "This is a direct translation of the [Text generation with a miniature GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) tutorial from Keras to JAX. It aims to teach developers who are familiar with Keras/Tensorflow to pick up JAX/Flax quickly.\n",
        "\n",
        "This notebook demonstrates how to use [Flax NNX](https://flax.readthedocs.io/en/latest/nnx/index.html) to implement an autoregressive language model using a miniaturized version of the GPT model. The model uses only a single transformer block and is easy to understand.\n",
        "\n",
        "It is assumed that Colab T4 is used to run this notebook. Adjust the batch size if another hardware is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTmz5Cbco7n_"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install JAX and Flax first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zMsOIc7ouCO",
        "outputId": "8d26acb0-5f86-4b57-c9e6-e46f372c52df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting jax-ai-stack\n",
            "  Downloading jax_ai_stack-2024.10.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: jax==0.4.33 in /usr/local/lib/python3.10/dist-packages (from jax-ai-stack) (0.4.33)\n",
            "Collecting flax==0.9.0 (from jax-ai-stack)\n",
            "  Downloading flax-0.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting ml-dtypes==0.4.0 (from jax-ai-stack)\n",
            "  Downloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: optax==0.2.3 in /usr/local/lib/python3.10/dist-packages (from jax-ai-stack) (0.2.3)\n",
            "Requirement already satisfied: orbax-checkpoint==0.6.4 in /usr/local/lib/python3.10/dist-packages (from jax-ai-stack) (0.6.4)\n",
            "Collecting orbax-export==0.0.5 (from jax-ai-stack)\n",
            "  Downloading orbax_export-0.0.5-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax==0.9.0->jax-ai-stack) (1.0.8)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax==0.9.0->jax-ai-stack) (0.1.66)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax==0.9.0->jax-ai-stack) (13.9.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax==0.9.0->jax-ai-stack) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax==0.9.0->jax-ai-stack) (6.0.2)\n",
            "Requirement already satisfied: jaxlib<=0.4.33,>=0.4.33 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.33->jax-ai-stack) (0.4.33)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.33->jax-ai-stack) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.33->jax-ai-stack) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.33->jax-ai-stack) (1.13.1)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax==0.2.3->jax-ai-stack) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax==0.2.3->jax-ai-stack) (0.1.87)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.10/dist-packages (from optax==0.2.3->jax-ai-stack) (1.9.4)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.6.4->jax-ai-stack) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.6.4->jax-ai-stack) (3.20.3)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.6.4->jax-ai-stack) (4.10.0)\n",
            "Collecting dataclasses-json (from orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting jaxtyping (from orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading jaxtyping-0.2.34-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax==0.2.3->jax-ai-stack) (0.12.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax==0.9.0->jax-ai-stack) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax==0.9.0->jax-ai-stack) (2.18.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint==0.6.4->jax-ai-stack) (2024.6.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint==0.6.4->jax-ai-stack) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint==0.6.4->jax-ai-stack) (3.20.2)\n",
            "Collecting typeguard==2.13.3 (from jaxtyping->orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax==0.9.0->jax-ai-stack) (0.1.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->orbax-export==0.0.5->jax-ai-stack) (24.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->orbax-export==0.0.5->jax-ai-stack)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading jax_ai_stack-2024.10.1-py3-none-any.whl (10 kB)\n",
            "Downloading flax-0.9.0-py3-none-any.whl (780 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.7/780.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orbax_export-0.0.5-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading jaxtyping-0.2.34-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: typeguard, mypy-extensions, ml-dtypes, marshmallow, typing-inspect, jaxtyping, dataclasses-json, orbax-export, flax, jax-ai-stack\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.3.0\n",
            "    Uninstalling typeguard-4.3.0:\n",
            "      Successfully uninstalled typeguard-4.3.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: flax\n",
            "    Found existing installation: flax 0.8.5\n",
            "    Uninstalling flax-0.8.5:\n",
            "      Successfully uninstalled flax-0.8.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.4.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 flax-0.9.0 jax-ai-stack-2024.10.1 jaxtyping-0.2.34 marshmallow-3.22.0 ml-dtypes-0.4.0 mypy-extensions-1.0.0 orbax-export-0.0.5 typeguard-2.13.3 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install jax-ai-stack\n",
        "!pip install -U \"jax[cuda12]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHzJ_bokoovZ"
      },
      "source": [
        "Grab the IMDB review data as the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olwq3MrpojcJ",
        "outputId": "c8a95b06-4d7d-4fc2-c6c7-558490b528c1"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  26.9M      0  0:00:02  0:00:02 --:--:-- 26.9M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKE2uUafLobI"
      },
      "source": [
        "Take care of the imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MKYFNOhdLq98"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "import optax\n",
        "from typing import Any\n",
        "import os\n",
        "import string\n",
        "import random\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPyt7MV6prz1"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Next, defne the model architecture, which is a decoder-only transformer model. The model is similar to the GPT model series but it's smaller in size with only one transformer block, which is why we are calling it miniGPT. The model has several key components stacked up together, so let's go over the them one by one.\n",
        "\n",
        "The key component is the `TransformerBlock`, which uses the multi-head attention mechanism as described in the famous [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper. Please get familiar with the paper if you are not already because we are going to implement some of the details below.\n",
        "\n",
        "The model is auto-regressive, so it can only attend to previous tokens. So we use [`jax.numpy.tril`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tril.html) to create the attention mask, and pass it in the `nnx.MultiHeadAttention` layer. The other layers follow the practice of the decoder layer in the paper.\n",
        "\n",
        "All layers (except `Dropout`) has a `rngs` parameter, which is the [random generator key](https://jax.readthedocs.io/en/latest/jax.random.html#prng-keys) that can help you reproduce results and debug issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z0p-IHurrB9i"
      },
      "outputs": [],
      "source": [
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):\n",
        "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads, in_features=embed_dim, rngs=rngs)\n",
        "        self.dropout1 = nnx.Dropout(rate=rate)\n",
        "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6, num_features=embed_dim, rngs=rngs)\n",
        "        self.linear1 = nnx.Linear(in_features=embed_dim, out_features=ff_dim, rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(in_features=ff_dim, out_features=embed_dim, rngs=rngs)\n",
        "        self.dropout2 = nnx.Dropout(rate=rate)\n",
        "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6, num_features=embed_dim, rngs=rngs)\n",
        "\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        batch_size, seq_len, _ = input_shape\n",
        "\n",
        "        # Create causal mask\n",
        "        mask = causal_attention_mask(seq_len)\n",
        "\n",
        "        # Apply MultiHeadAttention with causal mask\n",
        "        attention_output = self.mha(\n",
        "            inputs_q=inputs,\n",
        "            mask=mask,\n",
        "            decode=False\n",
        "        )\n",
        "        attention_output = self.dropout1(attention_output, deterministic=not training)\n",
        "        out1 = self.layer_norm1(inputs + attention_output)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_output = self.linear1(out1)\n",
        "        ffn_output = nnx.relu(ffn_output)\n",
        "        ffn_output = self.linear2(ffn_output)\n",
        "        ffn_output = self.dropout2(ffn_output, deterministic=not training)\n",
        "\n",
        "        return self.layer_norm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVA647SA8mQT"
      },
      "source": [
        "Since the model input is just text tokens, we need to convert them into embeddings. We use two kinds of embeddings: token embedding and position embeddings, both of which are learned by the model and are added up. Note that this is slightly different from the origianl paper, which uses static, instead of learned, positional embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ywxWh4cg5Kh2"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        token_embedding = self.token_emb(x)\n",
        "        return token_embedding + position_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUTg9IxJ8-Q1"
      },
      "source": [
        "Now we can put everything together to build our miniGPT model. We convert the tokens into embeddings, add a single `TransformerBlock` and finally use a linear projection layer for output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YmUaAvr75SvU"
      },
      "outputs": [],
      "source": [
        "class MiniGPT(nnx.Module):\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, feed_forward_dim: int, *, rngs: nnx.Rngs):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "                    maxlen, vocab_size, embed_dim, rngs=rngs\n",
        "                )\n",
        "        self.transformer_block = TransformerBlock(\n",
        "            embed_dim, num_heads, feed_forward_dim, rngs=rngs\n",
        "        )\n",
        "        self.output_layer = nnx.Linear(in_features=embed_dim, out_features=vocab_size, rngs=rngs)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        x = self.transformer_block(x, training=training)\n",
        "        outputs = self.output_layer(x)\n",
        "        return outputs\n",
        "\n",
        "def create_model(rngs):\n",
        "    return MiniGPT(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, rngs=rngs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igX_eoGNMTGR"
      },
      "source": [
        "Set some hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GRhiDsCrMZRp"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "maxlen = 80\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "feed_forward_dim = 256\n",
        "batch_size = 512 # for Colab T4 GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI1ci-HyMspJ"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "Data loading and preprocessing. To map the words and symbols to indices, we need to tokenize them first. For simplicity, we are using a vey simple tokenization scheme:\n",
        "* The `custom_standardization` function does some preprocessing by removing undesirable symbols and adding space before punctuations, so that punctuations can be treated as tokens like words\n",
        "* The `build_vocab` function builds our own vocaulary according to the `vocab_size` defined above\n",
        "* The `tokenize` function does the tokenization\n",
        "* We also batch the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGUFsn1GMuzh",
        "outputId": "c855e033-e8d7-42db-f027-a7a1220f2833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000 files\n"
          ]
        }
      ],
      "source": [
        "# Data loading and preprocessing\n",
        "filenames = []\n",
        "directories = [\n",
        "    \"./aclImdb/train/pos\",\n",
        "    \"./aclImdb/train/neg\",\n",
        "    \"./aclImdb/test/pos\",\n",
        "    \"./aclImdb/test/neg\",\n",
        "]\n",
        "for dir in directories:\n",
        "    for f in os.listdir(dir):\n",
        "        filenames.append(os.path.join(dir, f))\n",
        "\n",
        "print(f\"{len(filenames)} files\")\n",
        "\n",
        "random.shuffle(filenames)\n",
        "\n",
        "# Custom text processing: add space before and after punctuations for tokenization\n",
        "def custom_standardization(input_string):\n",
        "    lowercased = input_string.lower()\n",
        "    stripped_html = lowercased.replace(\"<br />\", \" \")\n",
        "    return ''.join([' ' + char + ' ' if char in string.punctuation else char for char in stripped_html]).strip()\n",
        "\n",
        "def build_vocab(texts, vocab_size):\n",
        "    all_words = ' '.join(texts).split()\n",
        "    word_counts = Counter(all_words)\n",
        "    vocab = ['<PAD>', '<UNK>'] + [word for word, _ in word_counts.most_common(vocab_size - 2)]\n",
        "    word_to_index = {word: index for index, word in enumerate(vocab)}\n",
        "    return vocab, word_to_index\n",
        "\n",
        "def tokenize(text, word_to_index, maxlen):\n",
        "    words = text.split()\n",
        "    tokens = [word_to_index.get(word, word_to_index['<UNK>']) for word in words]\n",
        "    if len(tokens) < maxlen:\n",
        "        tokens = tokens + [word_to_index['<PAD>']] * (maxlen - len(tokens))\n",
        "    else:\n",
        "        tokens = tokens[:maxlen]\n",
        "    return tokens\n",
        "\n",
        "def load_and_preprocess_data(filenames, batch_size, vocab_size, maxlen):\n",
        "    data = []\n",
        "    for filename in filenames:\n",
        "        with open(filename, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            processed_text = custom_standardization(text)\n",
        "            data.append(processed_text)\n",
        "\n",
        "    vocab, word_to_index = build_vocab(data, vocab_size)\n",
        "    tokenized_data = [tokenize(text, word_to_index, maxlen) for text in data]\n",
        "\n",
        "    # Batch the data\n",
        "    batched_data = [tokenized_data[i:i+batch_size] for i in range(0, len(tokenized_data), batch_size)]\n",
        "\n",
        "    return batched_data, vocab, word_to_index\n",
        "\n",
        "text_ds, vocab, word_to_index = load_and_preprocess_data(filenames, batch_size, vocab_size, maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKVSD8KSM1um"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Define a helper function for generating text given a model and prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_f4rEMm4M5lg"
      },
      "outputs": [],
      "source": [
        "def generate_text(model: MiniGPT, max_tokens: int, start_tokens: [int], index_to_word: [str], top_k=10):\n",
        "    def sample_from(logits):\n",
        "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "        logits = nnx.softmax(logits)\n",
        "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
        "\n",
        "    def generate_step(start_tokens):\n",
        "        pad_len = maxlen - len(start_tokens)\n",
        "        sample_index = len(start_tokens) - 1\n",
        "        if pad_len < 0:\n",
        "            x = jnp.array(start_tokens[:maxlen])\n",
        "            sample_index = maxlen - 1\n",
        "        elif pad_len > 0:\n",
        "            x = jnp.array(start_tokens + [0] * pad_len)\n",
        "        else:\n",
        "            x = jnp.array(start_tokens)\n",
        "\n",
        "        x = x[None, :]\n",
        "        logits = model(x)\n",
        "        next_token = sample_from(logits[0][sample_index])\n",
        "        return next_token\n",
        "\n",
        "    generated = []\n",
        "    for _ in range(max_tokens):\n",
        "        next_token = generate_step(start_tokens + generated)\n",
        "        generated.append(int(next_token))\n",
        "    return \" \".join([index_to_word[token] for token in start_tokens + generated])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkuaFXkANFNp"
      },
      "source": [
        "Define the loss function and training step function. The `train_step` is usually the most expensive function since it needs to compute the gradients and update the model parameters. We can use [JAX JIT compilation](https://jax.readthedocs.io/en/latest/jit-compilation.html#jit-compiling-a-function) to accelerate the execution of this function, but since we using NNX here, we annoate it with `@nnx.jit` instead of `@jax.jit`. JIT-compiled functions sometimes are tricky to debug; please refer to our [debugging documentation](https://jax.readthedocs.io/en/latest/debugging/print_breakpoint.html#compiled-prints-and-breakpoints) for help if you encouter such a situation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8rRuTmABNV4b"
      },
      "outputs": [],
      "source": [
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
        "    return loss, logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model: MiniGPT, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    optimizer.update(grads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5um2vkeUNckm"
      },
      "source": [
        "Start training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ysl6CsfENeJN",
        "outputId": "e70e6f9c-edbc-4b20-f2c4-e81d8f4f4e82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial generated text:\n",
            "this movie is seem weary thugs wrestler breckinridge charmer homosexual flu mia comeuppance luchino pioneer denominator globus burnt inside frying blackie surfaces 36 specific lung sophomore feminists tact filled exploitative minus standouts equation effectively plagued chauffeur muriel compton english ratso shifty blowing nosed\n",
            "\n",
            "Epoch 1, Loss: 6.123619079589844\n",
            "Generated text:\n",
            "this movie is not that the best films . . the acting was very bad movies , the story of the story . i would be the story was very well as i can be the story . . . . i have\n",
            "\n",
            "Epoch 2, Loss: 5.0509748458862305\n",
            "Generated text:\n",
            "this movie is the best movies i can say i am a huge fan of my opinion , and i thought that this film is so many people in this film is so many times , and it was very much more about\n",
            "\n",
            "Epoch 3, Loss: 4.757748603820801\n",
            "Generated text:\n",
            "this movie is the best film that it ' s just as i was going to watch this film , i have ever seen in my time . it was not so much better . the plot was not funny and it .\n",
            "\n",
            "Epoch 4, Loss: 4.550836086273193\n",
            "Generated text:\n",
            "this movie is the best movie ever . i can ' t understand that this film was just about a good actor and his performance in his <UNK> <UNK> <UNK> ( played by <UNK> <UNK> ) . i am a big fan of\n",
            "\n",
            "Epoch 5, Loss: 4.408228874206543\n",
            "Generated text:\n",
            "this movie is one of the best of the best of the worst films i ' ve ever seen . the plot was terrible , but it ' s just the first half hour , but the plot was very well , but\n",
            "\n",
            "Epoch 6, Loss: 4.29685115814209\n",
            "Generated text:\n",
            "this movie is one of the best films of the first film , it is the worst of all i ' ve ever seen . the film has no plot whatsoever , the story of the story , the plot was the story\n",
            "\n",
            "Epoch 7, Loss: 4.203516006469727\n",
            "Generated text:\n",
            "this movie is one of the best films of all time , and it is so far . the only thing i was surprised by it . it ' s the worst film i ' ve ever seen in the theater , but\n",
            "\n",
            "Epoch 8, Loss: 4.1221699714660645\n",
            "Generated text:\n",
            "this movie is one of the best of all time , it is so far and i have to say i am very impressed . the acting was so good and it ' s just plain awful . the movie was awful .\n",
            "\n",
            "Epoch 9, Loss: 4.049474716186523\n",
            "Generated text:\n",
            "this movie is one of the best of all time , it is so far and i have to say i am very impressed with this one . the story is about the main characters in this film , the first movie ,\n",
            "\n",
            "Epoch 10, Loss: 3.983396530151367\n",
            "Generated text:\n",
            "this movie is one of the best of all time , and it is so good , and the plot is not very much for a good time . i don ' t understand how it can you have to feel it '\n",
            "\n",
            "Epoch 11, Loss: 3.9226903915405273\n",
            "Generated text:\n",
            "this movie is one of the best films i ' ve ever seen . the only thing i have to say , \" this is a bad movie that has the worst plot of all i have ever seen . it ' s\n",
            "\n",
            "Epoch 12, Loss: 3.866518259048462\n",
            "Generated text:\n",
            "this movie is one of the best films i ' ve ever seen . i don ' t think the movie was so good in a long time , and it was so good , i would say it was not disappointed .\n",
            "\n",
            "Epoch 13, Loss: 3.8136610984802246\n",
            "Generated text:\n",
            "this movie is about a boy named john , who gets his girl . she gets her a girl and she finds a lot to her . . . she ' s also directed a very well written screenplay and directed by robert\n",
            "\n",
            "Epoch 14, Loss: 3.7642722129821777\n",
            "Generated text:\n",
            "this movie is about a very touching and heartwarming . it is about a girl , she is pregnant , but her boyfriend , is not very handsome and her sister , but her boyfriend has always been <UNK> for a movie ,\n",
            "\n",
            "Epoch 15, Loss: 3.7181167602539062\n",
            "Generated text:\n",
            "this movie is one of the best films i have ever seen . i am a huge fan of <UNK> , but this one was not only because of the story , it was so funny , but i thought i ' d\n",
            "\n",
            "Epoch 16, Loss: 3.674254894256592\n",
            "Generated text:\n",
            "this movie is one of the worst movies i have ever seen . the first time i was very excited about this show , but it ' s just not that it ' s just plain awful . it ' s just plain\n",
            "\n",
            "Epoch 17, Loss: 3.631474256515503\n",
            "Generated text:\n",
            "this movie is one of the worst movies i have ever seen . it ' s just bad . the plot is ridiculous , the plot is predictable . i have no idea of a bunch of other animals that they were <UNK>\n",
            "\n",
            "Epoch 18, Loss: 3.5906691551208496\n",
            "Generated text:\n",
            "this movie is about a bunch of guys and dolls ( with <UNK> , and their own , i mean ) i would not have seen this film , but it was just too long and boring . the film was just so\n",
            "\n",
            "Epoch 19, Loss: 3.5525403022766113\n",
            "Generated text:\n",
            "this movie is about a bunch of <UNK> guys and dolls chucky ' s back in his career . i don ' t care much about it . but , it is so funny and entertaining . . . . . . .\n",
            "\n",
            "Epoch 20, Loss: 3.5167582035064697\n",
            "Generated text:\n",
            "this movie is not bad , it is the best of the movie i have ever seen . it ' s the acting was awful , and i would say , and the movie is just so stupid , but the actors do\n",
            "\n",
            "Epoch 21, Loss: 3.482971429824829\n",
            "Generated text:\n",
            "this movie is so bad i can say is . it really isn ' t even that good , it ' s just plain awful . the plot is so predictable that it is so laughable . it is a spoof that the\n",
            "\n",
            "Epoch 22, Loss: 3.4505770206451416\n",
            "Generated text:\n",
            "this movie is so bad . i have never seen the first movie . it is the best thing i ' ve ever seen ! i am sure it ' s the first film i saw . the story and it ' s\n",
            "\n",
            "Epoch 23, Loss: 3.419351577758789\n",
            "Generated text:\n",
            "this movie is about a woman who pretends to be baby to be her father , the father , a boy who wants to see him and gets his job in his mother , who has to marry , but then , and\n",
            "\n",
            "Epoch 24, Loss: 3.3895630836486816\n",
            "Generated text:\n",
            "this movie is not very bad . i don ' t think i would say , it was the best film ever . it ' s the acting is great . i was really surprised at how the story unfolded i knew were\n",
            "\n",
            "Epoch 25, Loss: 3.363539218902588\n",
            "Generated text:\n",
            "this movie is one of the best i have ever seen . i am a fan of <UNK> ' s book and <UNK> <UNK> <UNK> <UNK> , a great actor . he has done the movie in this film . i am very\n",
            "\n",
            "Final generated text:\n",
            "this movie is one of the best i have ever seen . i am a fan of <UNK> ' s book and <UNK> <UNK> <UNK> <UNK> , a great actor . he has done the movie in this film . i am very\n"
          ]
        }
      ],
      "source": [
        "model = create_model(rngs=nnx.Rngs(0))\n",
        "optimizer = nnx.Optimizer(model, optax.adam(1e-3))\n",
        "metrics = nnx.MultiMetric(\n",
        "  loss=nnx.metrics.Average('loss'),\n",
        "  # You can add additional metrics for tracking\n",
        ")\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "start_prompt = \"this movie is\"\n",
        "start_tokens = [word_to_index.get(word, word_to_index['<UNK>']) for word in start_prompt.split()]\n",
        "index_to_word = {i: word for word, i in word_to_index.items()}\n",
        "generated_text = generate_text(\n",
        "    model, 40, start_tokens, index_to_word\n",
        ")\n",
        "print(f\"Initial generated text:\\n{generated_text}\\n\")\n",
        "\n",
        "num_epochs = 25\n",
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in text_ds:\n",
        "        input_batch = jnp.array(batch)\n",
        "        target_batch = jnp.array([tokens[1:] + [word_to_index['<PAD>']] for tokens in batch])\n",
        "        train_step(model, optimizer, metrics, (input_batch, target_batch))\n",
        "\n",
        "    for metric, value in metrics.compute().items():  # compute metrics\n",
        "      metrics_history[f'train_{metric}'].append(value)  # record metrics\n",
        "    metrics.reset()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {metrics_history['train_loss'][-1]}\")\n",
        "    start_prompt = \"this movie is\"\n",
        "    start_tokens = [word_to_index.get(word, word_to_index['<UNK>']) for word in start_prompt.split()]\n",
        "    generated_text = generate_text(\n",
        "        model, 40, start_tokens, index_to_word\n",
        "    )\n",
        "    print(f\"Generated text:\\n{generated_text}\\n\")\n",
        "\n",
        "# Final text generation\n",
        "start_tokens = [word_to_index.get(word, word_to_index['<UNK>']) for word in start_prompt.split()]\n",
        "generated_text = generate_text(\n",
        "    model, 40, start_tokens, index_to_word\n",
        ")\n",
        "print(f\"Final generated text:\\n{generated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB-ExEt1Zl1C"
      },
      "source": [
        "As you can see, the model goes from generating completely random words at the beginning to generating sentences that look like sensible movie reviews at the end of the training. Of course the reviews are far from perfect because this model is really small and fundamentally lacks strong intelligence like modern LLMs. In our next tutorial, we are going to scale the model up and make it smarter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pKD3c1UMu7x"
      },
      "source": [
        "## Save the model\n",
        "\n",
        "We use [Orbax](https://github.com/google/orbax) to save the model checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkoFGCgSZ1yz",
        "outputId": "71d0e320-be56-4c61-88cb-ff119df01813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_CHECKPOINT_METADATA  d  manifest.ocdbt  _METADATA  ocdbt.process_0  _sharding\n"
          ]
        }
      ],
      "source": [
        "import orbax.checkpoint as orbax\n",
        "\n",
        "state = nnx.state(model)\n",
        "\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "checkpointer.save('/content/save', state)\n",
        "\n",
        "# Make sure the files are there\n",
        "!ls /content/save/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3Zrue6HWMwkG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}